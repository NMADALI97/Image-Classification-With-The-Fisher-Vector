{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fisher Vector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlWU-sTa6RqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import sys, glob, argparse\n",
        "import numpy as np\n",
        "import math, cv2\n",
        "from scipy.stats import multivariate_normal\n",
        "import time\n",
        "from sklearn import svm\n",
        "\n",
        "def dictionary(descriptors, N):\n",
        "\tem = cv2.EM(N)\n",
        "\tem.train(descriptors)\n",
        "\n",
        "\treturn np.float32(em.getMat(\"means\")), \\\n",
        "\t\tnp.float32(em.getMatVector(\"covs\")), np.float32(em.getMat(\"weights\"))[0]\n",
        "\n",
        "def image_descriptors(file):\n",
        "\timg = cv2.imread(file, 0)\n",
        "\timg = cv2.resize(img, (256, 256))\n",
        "\t_ , descriptors = cv2.SIFT().detectAndCompute(img, None)\n",
        "\treturn descriptors\n",
        "\n",
        "def folder_descriptors(folder):\n",
        "\tfiles = glob.glob(folder + \"/*.jpg\")\n",
        "\tprint(\"Calculating descriptos. Number of images is\", len(files))\n",
        "\treturn np.concatenate([image_descriptors(file) for file in files])\n",
        "\n",
        "def likelihood_moment(x, ytk, moment):\t\n",
        "\tx_moment = np.power(np.float32(x), moment) if moment > 0 else np.float32([1])\n",
        "\treturn x_moment * ytk\n",
        "\n",
        "def likelihood_statistics(samples, means, covs, weights):\n",
        "\tgaussians, s0, s1,s2 = {}, {}, {}, {}\n",
        "\tsamples = zip(range(0, len(samples)), samples)\n",
        "\t\n",
        "\tg = [multivariate_normal(mean=means[k], cov=covs[k]) for k in range(0, len(weights)) ]\n",
        "\tfor index, x in samples:\n",
        "\t\tgaussians[index] = np.array([g_k.pdf(x) for g_k in g])\n",
        "\n",
        "\tfor k in range(0, len(weights)):\n",
        "\t\ts0[k], s1[k], s2[k] = 0, 0, 0\n",
        "\t\tfor index, x in samples:\n",
        "\t\t\tprobabilities = np.multiply(gaussians[index], weights)\n",
        "\t\t\tprobabilities = probabilities / np.sum(probabilities)\n",
        "\t\t\ts0[k] = s0[k] + likelihood_moment(x, probabilities[k], 0)\n",
        "\t\t\ts1[k] = s1[k] + likelihood_moment(x, probabilities[k], 1)\n",
        "\t\t\ts2[k] = s2[k] + likelihood_moment(x, probabilities[k], 2)\n",
        "\n",
        "\treturn s0, s1, s2\n",
        "\n",
        "def fisher_vector_weights(s0, s1, s2, means, covs, w, T):\n",
        "\treturn np.float32([((s0[k] - T * w[k]) / np.sqrt(w[k]) ) for k in range(0, len(w))])\n",
        "\n",
        "def fisher_vector_means(s0, s1, s2, means, sigma, w, T):\n",
        "\treturn np.float32([(s1[k] - means[k] * s0[k]) / (np.sqrt(w[k] * sigma[k])) for k in range(0, len(w))])\n",
        "\n",
        "def fisher_vector_sigma(s0, s1, s2, means, sigma, w, T):\n",
        "\treturn np.float32([(s2[k] - 2 * means[k]*s1[k]  + (means[k]*means[k] - sigma[k]) * s0[k]) / (np.sqrt(2*w[k])*sigma[k])  for k in range(0, len(w))])\n",
        "\n",
        "def normalize(fisher_vector):\n",
        "\tv = np.sqrt(abs(fisher_vector)) * np.sign(fisher_vector)\n",
        "\treturn v / np.sqrt(np.dot(v, v))\n",
        "\n",
        "def fisher_vector(samples, means, covs, w):\n",
        "\ts0, s1, s2 =  likelihood_statistics(samples, means, covs, w)\n",
        "\tT = samples.shape[0]\n",
        "\tcovs = np.float32([np.diagonal(covs[k]) for k in range(0, covs.shape[0])])\n",
        "\ta = fisher_vector_weights(s0, s1, s2, means, covs, w, T)\n",
        "\tb = fisher_vector_means(s0, s1, s2, means, covs, w, T)\n",
        "\tc = fisher_vector_sigma(s0, s1, s2, means, covs, w, T)\n",
        "\tfv = np.concatenate([np.concatenate(a), np.concatenate(b), np.concatenate(c)])\n",
        "\tfv = normalize(fv)\n",
        "\treturn fv\n",
        "\n",
        "def generate_gmm(input_folder, N):\n",
        "\twords = np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
        "\tprint(\"Training GMM of size\", N)\n",
        "\tmeans, covs, weights = dictionary(words, N)\n",
        "\t#Throw away gaussians with weights that are too small:\n",
        "\tth = 1.0 / N\n",
        "\tmeans = np.float32([m for k,m in zip(range(0, len(weights)), means) if weights[k] > th])\n",
        "\tcovs = np.float32([m for k,m in zip(range(0, len(weights)), covs) if weights[k] > th])\n",
        "\tweights = np.float32([m for k,m in zip(range(0, len(weights)), weights) if weights[k] > th])\n",
        "\n",
        "\tnp.save(\"means.gmm\", means)\n",
        "\tnp.save(\"covs.gmm\", covs)\n",
        "\tnp.save(\"weights.gmm\", weights)\n",
        "\treturn means, covs, weights\n",
        "\n",
        "def get_fisher_vectors_from_folder(folder, gmm):\n",
        "\tfiles = glob.glob(folder + \"/*.jpg\")\n",
        "\treturn np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
        "\n",
        "def fisher_features(folder, gmm):\n",
        "\tfolders = glob.glob(folder + \"/*\")\n",
        "\tfeatures = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
        "\treturn features\n",
        "\n",
        "def train(gmm, features):\n",
        "\tX = np.concatenate(features.values())\n",
        "\tY = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
        "\n",
        "\tclf = svm.SVC()\n",
        "\tclf.fit(X, Y)\n",
        "\treturn clf\n",
        "\n",
        "def success_rate(classifier, features):\n",
        "\tprint(\"Applying the classifier...\")\n",
        "\tX = np.concatenate(np.array(features.values()))\n",
        "\tY = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
        "\tres = float(sum([a==b for a,b in zip(classifier.predict(X), Y)])) / len(Y)\n",
        "\treturn res\n",
        "\t\n",
        "def load_gmm(folder = \"\"):\n",
        "\tfiles = [\"means.gmm.npy\", \"covs.gmm.npy\", \"weights.gmm.npy\"]\n",
        "\treturn map(lambda file: load(file), map(lambda s : folder + \"/\" , files))\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-d' , \"--dir\", help=\"Directory with images\" , default='.')\n",
        "    parser.add_argument(\"-g\" , \"--loadgmm\" , help=\"Load Gmm dictionary\", action = 'store_true', default = False)\n",
        "    parser.add_argument('-n' , \"--number\", help=\"Number of words in dictionary\" , default=5, type=int)\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXvvth_S6U5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = get_args()\n",
        "working_folder = args.dir\n",
        "\n",
        "gmm = load_gmm(working_folder) if args.loadgmm else generate_gmm(working_folder, args.number)\n",
        "fisher_features = fisher_features(working_folder, gmm)\n",
        "#TBD, split the features into training and validation\n",
        "classifier = train(gmm, fisher_features)\n",
        "rate = success_rate(classifier, fisher_features)\n",
        "print(\"Success rate is\", rate)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}